<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Hadoop SequenceFile I/O 流分析]]></title>
    <url>%2F2017%2F08%2F25%2Fhadoop-sequencefile-class-analysis%2F</url>
    <content type="text"><![CDATA[由于SequenceFile中各种Stream相互嵌套，初学者很难理清各个Stream之间的关系，本文将直接展示Writer和Reader构造并且初始化完成后，最终构成层层类Stream字段的实际对象。本文将以RecordCompressWriter为例。 1 Writer类12345678910111213141516171819202122class DataOutputBuffer extends DataOutputStream &#123; class Buffer extends ByteArrayOutputStream &#123; Buffer() &#123; ByteArrayOutputStream() &#123; ByteArrayOutputStream(32) extends OutputStream &#123; protected byte buf[] = new byte[32]; &#125;; &#125;; &#125;; &#125; DataOutputBuffer() &#123; Buffer buffer = new Buffer(); DataOutputBuffer(buffer) &#123; DataOutputStream(buffer) extends FilterOutputStream &#123; FilterOutputStream(buffer) extends OutputStream &#123; protected OutputStream out = buffer; &#125;; &#125;; private Buffer buffer = buffer; &#125;; &#125;;&#125; 12345678910111213141516171819202122232425class FSDataOutputStream extends DataOutputStream &#123; LocalFSFileOutputStream localFSFileOutputStream = new LocalFSFileOutputStream(..) extends OutputStream &#123; private FileOutputStream fos = new FileOutputStream(..) extends OutputStream &#123; &#125;; &#125;; BufferedOutputStream bufferedOutputStream = new BufferedOutputStream(localFSFileOutputStream, ..) extends FilterOutputStream &#123; FilterOutputStream(localFSFileOutputStream) extends OutputStream &#123; protected OutputStream out = localFSFileOutputStream; &#125;; protected byte buf[] = new byte[4096]; &#125;; FSDataOutputStream(bufferedOutputStream, ..) &#123; PositionCache positionCache = new PositionCache(bufferedOutputStream, ..) extends FilterOutputStream &#123; FilterOutputStream(bufferedOutputStream) &#123; protected OutputStream out = bufferedOutputStream; &#125;; &#125;; DataOutputStream(positionCache) extends FilterOutputStream &#123; FilterOutputStream(positionCache) &#123; protected OutputStream out = positionCache; &#125;; &#125;; private final OutputStream wrappedStream = bufferedOutputStream; &#125;;&#125; 12345678910111213141516171819class JavaSerialization &#123; class JavaSerializationSerializer &#123; open(buffer) &#123; // DataOutputBuffer ObjectOutputStream objectOutputStream = new ObjectOutputStream(buffer) extends OutputStream &#123; BlockDataOutputStream blockDataOutputStream = new BlockDataOutputStream(buffer) extends OutputStream &#123; private final OutputStream out = buffer; DataOutputStream dataOutputStream = new DataOutputStream(blockDataOutputStream) extends FilterOutputStream &#123; FilterOutputStream(blockDataOutputStream) extends OutputStream &#123; protected OutputStream out = blockDataOutputStream; &#125;; &#125;; private final DataOutputStream dout = dataOutputStream; &#125;; private final BlockDataOutputStream bout = blockDataOutputStream; &#125;; private ObjectOutputStream oos = objectOutputStream; &#125;; &#125;&#125; 1234567891011class DefaultCodec &#123; CompressionOutputStream createOutputStream(buffer, compressor) &#123; //DataOutputBuffer, BuiltInZlibDeflater CompressorStream(buffer, compressor) extends CompressionOutputStream &#123; CompressionOutputStream(buffer) extends OutputStream &#123; protected final OutputStream out = buffer; &#125;; protected Compressor compressor = compressor; protected byte[] buffer = new byte[4096]; &#125;; &#125;;&#125; 123456789101112131415class DataOutputStream extends FilterOutputStream &#123; BufferedOutputStream bufferedOutputStream = BufferedOutputStream(deflateFilter) &#123; // CompressorStream BufferedOutputStream(deflateFilter, ..) extends FilterOutputStream &#123; FilterOutputStream(deflateFilter) extends OutputStream &#123; protected OutputStream out = deflateFilter; &#125;; protected byte buf[] = new byte[8192]; &#125;; &#125;; DataOutputStream(bufferedOutputStream) &#123; FilterOutputStream(bufferedOutputStream) extends OutputStream &#123; protected OutputStream out = bufferedOutputStream; &#125;; &#125;;&#125; 12345678910111213141516171819class JavaSerialization &#123; class JavaSerializationSerializer &#123; open(deflateOut) &#123; // DataOutputStream ObjectOutputStream objectOutputStream = new ObjectOutputStream(deflateOut) extends OutputStream &#123; BlockDataOutputStream blockDataOutputStream = new BlockDataOutputStream(deflateOut) extends OutputStream &#123; private final OutputStream out = deflateOut; DataOutputStream dataOutputStream = new DataOutputStream(blockDataOutputStream) extends FilterOutputStream &#123; FilterOutputStream(blockDataOutputStream) extends OutputStream &#123; protected OutputStream out = blockDataOutputStream; &#125;; &#125;; private final DataOutputStream dout = dataOutputStream; &#125;; private final BlockDataOutputStream bout = blockDataOutputStream; &#125;; private ObjectOutputStream oos = objectOutputStream; &#125;; &#125;&#125; 1234567891011class Writer &#123; FSDataOutputStream out; // FSDataOutputStream DataOutputBuffer buffer = new DataOutputBuffer(); // DataOutputBuffer CompressionCodec codec = null; // DefaultCodec CompressionOutputStream deflateFilter = null; // CompressorStream DataOutputStream deflateOut = null; // DataOutputStream Compressor compressor = null; // BuiltInZlibDeflater protected Serializer keySerializer; // JavaSerialization$JavaSerializationSerializer protected Serializer uncompressedValSerializer; // JavaSerialization$JavaSerializationSerializer protected Serializer compressedValSerializer; // JavaSerialization$JavaSerializationSerializer&#125;]]></content>
      <categories>
        <category>Java</category>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Java</tag>
        <tag>I/O</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop ObjectWritable 源码与相关单元测试介绍]]></title>
    <url>%2F2017%2F07%2F29%2Fhadoop-objectwritable-class-analysis%2F</url>
    <content type="text"><![CDATA[Hadoop IO模块是序列化和RPC的基础，该模块负责将java的各种对象转化为byte流以便在网络上传输，并且还要负责将byte流转回原对象。开篇我们先看一下hadoop IO模块类的关系图： 其他的很好理解，每种java的原生的类和容器都有一个对应实现了Writable的类（IntWritable、MapFile等）。ObjectWritable 是hadoop.io模块里面一个比较特殊的类（用红色标出），它是一个通用的Writable类，可以将任意的自定义类转为byte流，原因是因为会它会把将要转换的对象的类信息记录在自己的成员变量private Class declaredClass;中，然后序列化时，将类的名字，以string的形式一并写入byte流中。在收到对应的byte流后，解析程序可以知道类名string的长度，然后从byte流中获得类的信息，之后就是根据类的信息去以此解析这个类的所有成员变量的值。 ObjectWritable是Writable的实现，必然要实现接口的方法： void write(DataOutput out) throws IOException;和void readFields(DataInput in) throws IOException;。ObjectWritable实现是通过调用类中的静态方法。 12345678@Overridepublic void write(DataOutput out) throws IOException &#123; writeObject(out, instance, declaredClass, conf);&#125;@Overridepublic void readFields(DataInput in) throws IOException &#123; readObject(in, this, this.conf);&#125; 所以ObjectWritable最主要的逻辑集中在这2个静态函数上，先看writeObject方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384 /** * Write a &#123;@link Writable&#125;, &#123;@link String&#125;, primitive type, or an array of * the preceding. * * @param allowCompactArrays - set true for RPC and internal or intra-cluster * usages. Set false for inter-cluster, File, and other persisted output * usages, to preserve the ability to interchange files with other clusters * that may not be running the same version of software. Sometime in ~2013 * we can consider removing this parameter and always using the compact format. * 入参包含要写入的流，要序列化的对象，对象的类信息，以及配置信息和是否允许封装数组 */ public static void writeObject(DataOutput out, Object instance, Class declaredClass, Configuration conf, boolean allowCompactArrays) throws IOException &#123; if (instance == null) &#123; // null instance = new NullInstance(declaredClass, conf); declaredClass = Writable.class; &#125; // Special case: must come before writing out the declaredClass. // If this is an eligible array of primitives, // wrap it in an ArrayPrimitiveWritable$Internal wrapper class. // // 当允许封装数组时，并且对象是一个只包含primitives的数组，就将对象数组包裹在 // ArrayPrimitiveWritable.Internal这个类中，同时修改对象的类信息为 // ArrayPrimitiveWritable.Internal.class if (allowCompactArrays &amp;&amp; declaredClass.isArray() &amp;&amp; instance.getClass().getName().equals(declaredClass.getName()) &amp;&amp; instance.getClass().getComponentType().isPrimitive()) &#123; instance = new ArrayPrimitiveWritable.Internal(instance); declaredClass = ArrayPrimitiveWritable.Internal.class; &#125; // 将类的名称写进byte流中：为了将来可以成功的将类的名称解析出来，writeString还会将 // string的长度以short类型写在名称之前 UTF8.writeString(out, declaredClass.getName()); // always write declared // 如果是数组，但是没有被封装在ArrayPrimitiveWritable.Internal类中，就递归调用 // writeObject写数组中的每一个元素 if (declaredClass.isArray()) &#123; // non-primitive or non-compact array int length = Array.getLength(instance); out.writeInt(length); for (int i = 0; i &lt; length; i++) &#123; writeObject(out, Array.get(instance, i), declaredClass.getComponentType(), conf, allowCompactArrays); &#125; // 如果数据已经被封装在了ArrayPrimitiveWritable.Internal中，那么就调用 // ArrayPrimitiveWritable.Internal的write方法 &#125; else if (declaredClass == ArrayPrimitiveWritable.Internal.class) &#123; ((ArrayPrimitiveWritable.Internal) instance).write(out); // 如果是String类型，调用UTF8的writeString法法 &#125; else if (declaredClass == String.class) &#123; // String UTF8.writeString(out, (String)instance); // 如果是基本类型，则直接调用DataOutput的相应write方法 &#125; else if (declaredClass.isPrimitive()) &#123; // primitive type if (declaredClass == Boolean.TYPE) &#123; // boolean out.writeBoolean(((Boolean)instance).booleanValue()); &#125; else if (declaredClass == Character.TYPE) &#123; // char out.writeChar(((Character)instance).charValue()); &#125; else if (declaredClass == Byte.TYPE) &#123; // byte out.writeByte(((Byte)instance).byteValue()); &#125; ...... else if (declaredClass == Void.TYPE) &#123; // void &#125; else &#123; throw new IllegalArgumentException("Not a primitive: "+declaredClass); &#125; &#125; else if (declaredClass.isEnum()) &#123; // enum UTF8.writeString(out, ((Enum)instance).name()); // 如果类是Writable的衍生类，则先将对象的类的名字写入byte流中，然后再调用Writable的write方法 // 由于在函数的开始，已经将declaredClass字符串写进了byte流，这里还会再写一次，不过这次的方法 // 写入的内容由instance.getClass()决定的，故可以推断存在可能declaredClass和 // instance.getClass()获得的内容不一样，需要再次将类型的string写入 &#125; else if (Writable.class.isAssignableFrom(declaredClass)) &#123; // Writable UTF8.writeString(out, instance.getClass().getName()); ((Writable)instance).write(out); &#125; else if (Message.class.isAssignableFrom(declaredClass)) &#123; ((Message)instance).writeDelimitedTo( DataOutputOutputStream.constructOutputStream(out)); &#125; else &#123; throw new IOException("Can't write: "+instance+" as "+declaredClass); &#125;&#125; 接下来看看ArrayPrimitiveWritable.Internal的write方法，逻辑很简单，1）先将数组中存储的基本类型名称以string的方式写到byte流中（包括类名称string的长度）；2）将数组中元素的个数以Int的方式写进流中；3）然后根据基本类型来调用相应的写数组方法（writeBooleanArray、writeIntArray等）将数组写进流中。writeIntArray等就是简单的遍历各个元素，然后调用DataOutput的write基本类型的方法writeInt。 123456789101112131415161718192021222324252627282930313233343536373839/* * @see org.apache.hadoop.io.Writable#write(java.io.DataOutput) */@Override@SuppressWarnings("deprecation")public void write(DataOutput out) throws IOException &#123; // write componentType UTF8.writeString(out, componentType.getName()); // write length out.writeInt(length); // do the inner loop. Walk the decision tree only once. if (componentType == Boolean.TYPE) &#123; // boolean writeBooleanArray(out); &#125; else if (componentType == Character.TYPE) &#123; // char writeCharArray(out); &#125; else if (componentType == Byte.TYPE) &#123; // byte writeByteArray(out); &#125; else if (componentType == Short.TYPE) &#123; // short writeShortArray(out); &#125; else if (componentType == Integer.TYPE) &#123; // int writeIntArray(out); &#125; else if (componentType == Long.TYPE) &#123; // long writeLongArray(out); &#125; else if (componentType == Float.TYPE) &#123; // float writeFloatArray(out); &#125; else if (componentType == Double.TYPE) &#123; // double writeDoubleArray(out); &#125; else &#123; throw new IOException("Component type " + componentType.toString() + " is set as the output type, but no encoding is implemented for this type."); &#125;&#125;private void writeIntArray(DataOutput out) throws IOException &#123; int[] v = (int[]) value; for (int i = 0; i &lt; length; i++) out.writeInt(v[i]);&#125; 至此就已经完成了将一个对象写到byte流中，接着看看从byte流中解析获得对象的过程readObject。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182/** Read a &#123;@link Writable&#125;, &#123;@link String&#125;, primitive type, or an array of * the preceding. */@SuppressWarnings("unchecked")public static Object readObject(DataInput in, ObjectWritable objectWritable, Configuration conf) throws IOException &#123; // 函数开始，就会尝试将对象的类名解析出来：byte流的头2个字节（short）表示了类的名称的string长度 // 然后根据这个长度读取接下来的一定字节数就可以成功的获得类名 String className = UTF8.readString(in); // 如果是基本类型，那就直接可以将类名的string转换为Java的类对象 Class&lt;?&gt; declaredClass = PRIMITIVE_NAMES.get(className); // 如果不是基本类型，则调用loadClass方法 if (declaredClass == null) &#123; declaredClass = loadClass(conf, className); &#125; Object instance; // 如果对象是基本类型，那就调用DataInput对应的read方法 if (declaredClass.isPrimitive()) &#123; // primitive types if (declaredClass == Boolean.TYPE) &#123; // boolean instance = Boolean.valueOf(in.readBoolean()); &#125; else if (declaredClass == Character.TYPE) &#123; // char instance = Character.valueOf(in.readChar()); &#125; else if (declaredClass == Byte.TYPE) &#123; // byte instance = Byte.valueOf(in.readByte()); &#125; else if (declaredClass == Short.TYPE) &#123; // short instance = Short.valueOf(in.readShort()); &#125; ...... &#125; else if (declaredClass == Void.TYPE) &#123; // void instance = null; &#125; else &#123; throw new IllegalArgumentException("Not a primitive: "+declaredClass); &#125; // 如果是没有被封装在ArrayPrimitiveWritable.Internal中的数组，就递归调用readObject // 依次读取数组的各个元素 &#125; else if (declaredClass.isArray()) &#123; // array int length = in.readInt(); instance = Array.newInstance(declaredClass.getComponentType(), length); for (int i = 0; i &lt; length; i++) &#123; Array.set(instance, i, readObject(in, conf)); &#125; // 如果被封装在了ArrayPrimitiveWritable.Internal类中，那么就调用 // ArrayPrimitiveWritable.Internal的readFields方法，然后保存在instance局部变量中 &#125; else if (declaredClass == ArrayPrimitiveWritable.Internal.class) &#123; // Read and unwrap ArrayPrimitiveWritable$Internal array. // Always allow the read, even if write is disabled by allowCompactArrays. ArrayPrimitiveWritable.Internal temp = new ArrayPrimitiveWritable.Internal(); temp.readFields(in); instance = temp.get(); declaredClass = instance.getClass(); // 其余的读取方法大同小异，都是找到该类对应的read方法 &#125; else if (declaredClass == String.class) &#123; // String instance = UTF8.readString(in); &#125; else if (declaredClass.isEnum()) &#123; // enum instance = Enum.valueOf((Class&lt;? extends Enum&gt;) declaredClass, UTF8.readString(in)); &#125; else if (Message.class.isAssignableFrom(declaredClass)) &#123; instance = tryInstantiateProtobuf(declaredClass, in); // 如果类型是Writable或Writable的衍生类，则执行下列代码 // 由于writeObject的write逻辑，将类型写入byte流了2次，故这里需要再次读取一下确定instance的类型 &#125; else &#123; // Writable Class instanceClass = null; String str = UTF8.readString(in); instanceClass = loadClass(conf, str); Writable writable = WritableFactories.newInstance(instanceClass, conf); writable.readFields(in); instance = writable; if (instanceClass == NullInstance.class) &#123; // null declaredClass = ((NullInstance)instance).declaredClass; instance = null; &#125; &#125; // 到这里可以将declaredClass和instance赋值给参数传入的ObjectWritable对象 if (objectWritable != null) &#123; // store values objectWritable.declaredClass = declaredClass; objectWritable.instance = instance; &#125; return instance; &#125; 解析过程到此就完成了。序列化和反序列化过程都是一一对应的。接下来就用Hadoop自带的测试用例来验证运行一下上述代码，看看写入byte流中的内容。测试类为TestArrayPrimitiveWritable。这个类主要测试了ArrayPrimitiveWritable的write和readFields方法。这2个方法基本覆盖了其他的Writable和其衍生类的读写功能。 123456789101112131415161718192021222324252627282930313233343536static final char[] c = &#123;'a', 'b', 'c'&#125;;static final int[] i = &#123;1, 2, 3&#125;;static final Object[] bigSet = &#123;c, i&#125;;static final Object[] expectedResultSet = &#123;c, c, i, i&#125;;@Testpublic void testMany() throws IOException &#123; //Write a big set of data, one of each primitive type array for (Object x : bigSet) &#123; //write each test object two ways //First, transparently via ObjectWritable ObjectWritable.writeObject(out, x, x.getClass(), null, true); //Second, explicitly via ArrayPrimitiveWritable (new ArrayPrimitiveWritable(x)).write(out); &#125; //Now read the data back in in.reset(out.getData(), out.getLength()); for (int x = 0; x &lt; resultSet.length; ) &#123; //First, transparently resultSet[x++] = ObjectWritable.readObject(in, null); //Second, explicitly ArrayPrimitiveWritable apw = new ArrayPrimitiveWritable(); apw.readFields(in); resultSet[x++] = apw.get(); &#125; //validate data structures and values assertEquals(expectedResultSet.length, resultSet.length); for (int x = 0; x &lt; resultSet.length; x++) &#123; assertEquals("ComponentType of array " + x, expectedResultSet[x].getClass().getComponentType(), resultSet[x].getClass().getComponentType()); &#125; assertTrue("In and Out arrays didn't match values", Arrays.deepEquals(expectedResultSet, resultSet));&#125; 下表详细列出了bigSet由ObjectWritable和ArrayPrimitiveWritable写入DataOutput中的数据。 起始位置 长度 内容 备注 0 2 52 字符数组被封装在ArrayPrimitiveWritable.Internal中，记录类全名字符串的长度 2 52 “org.apache.hadoop.io. ArrayPrimitiveWritable$Internal” declaredClass.getName() 54 2 4 数组元素为char型，”char”字符串的长度 56 4 “char” 数组的基本元素类型”char”字符串 60 4 3 数组总共有3个元素’a’、’b’、’c’，记录数组元素的个数，也就是数组长度 64 2 ‘a’ 第一个元素’a’ 66 2 ‘b’ 第二个元素’b’ 68 2 ‘c’ 第三个元素’c’ ObjectWritable.writeObject写char数组结束，接下来是直接调用ArrayPrimitiveWritable来写char数组 70 2 4 数组元素为char型，”char”字符串的长度 72 4 “char” 数组的基本元素类型”char”字符串 76 4 3 数组总共有3个元素’a’、’b’、’c’，记录数组元素的个数，也就是数组长度 80 2 ‘a’ 第一个元素’a’ 82 2 ‘b’ 第二个元素’b’ 84 2 ‘c’ 第三个元素’c’ ArrayPrimitiveWritable写char数组完成，开始用ObjectWritable.writeObject写Int数组 86 2 52 Int数组同样的被封装在ArrayPrimitiveWritable.Internal中，记录类全名字符串的长度 88 52 “org.apache.hadoop.io. ArrayPrimitiveWritable$Internal” declaredClass.getName() 140 2 3 Int数组元素为int型，”int”字符串的长度 142 3 “int” 数组的基本元素类型”int”字符串 145 4 3 数组总共有3个元素1、2、3，记录数组元素的个数，也就是数组长度 149 4 1 第一个元素1 153 4 2 第二个元素2 157 4 3 第三个元素3 ObjectWritable.writeObject写Int数组结束，接下来是直接调用ArrayPrimitiveWritable来写Int数组 161 2 3 Int数组元素为int型，”int”字符串的长度 163 3 “int” 数组的基本元素类型”int”字符串 166 4 3 数组总共有3个元素1、2、3，记录数组元素的个数，也就是数组长度 170 4 1 第一个元素1 174 4 2 第二个元素2 178 4 3 第三个元素3 写入流程结束，DataOutput中总共有182个字节数据 将这个182个字节传递给DataInput对象，然后分别用ObjectWritable.readObject和ArrayPrimitiveWritable处理，可以准确的将2个字符串的2遍都读取出来，最后放进resultSet中，将resultSet和expectedResultSet对比，可以发现最终结果和期望的结果一致，表明解析数组成功了。 从这182字节的内容，我们可以看出ObjectWritable.writeObject比ArrayPrimitiveWritable只是多了传递给ObjectWritable.writeObject的declaredClass字符串的长度，其他都是一样的，调用ObjectWritable.writeObject会隐式调用ArrayPrimitiveWritable的write方法。 这个测试用例只测试了ObjectWritable.writeObject处理元素为基本类型的数组，并没有测试处理Writable类型的对象。对于Writable对象，其本身已经包含了序列化的函数write和readFields，并不需要借助ObjectWritable来实现序列化，但ObjectWritable可以处理任何类的序列化需求，当然也不排除处理Writable及它衍生类。 下面一个测试用例，可以用来检验ObjectWritable.writeObject处理Writable类型的对象。首先，使用ObjectWritable.writeObject直接序列化一个Int数组，之后，用ArrayPrimitiveWritable封装这个Int数组，再用ObjectWritable.writeObject序列化封装过后的这个数组。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152static final int[] i = &#123;1, 2, 3&#125;;@Test@SuppressWarnings("deprecation")public void testObjectLabeling() throws IOException &#123; //Do a few tricky experiments to make sure things are being written //the way we expect //Write the data array with ObjectWritable //which will indirectly write it using APW.Internal ObjectWritable.writeObject(out, i, i.getClass(), null, true); //Write the corresponding APW directly with ObjectWritable ArrayPrimitiveWritable apw = new ArrayPrimitiveWritable(i); ObjectWritable.writeObject(out, apw, apw.getClass(), null, true); //Get ready to read it back in.reset(out.getData(), out.getLength()); //Read the int[] object as written by ObjectWritable, but //"going around" ObjectWritable String className = UTF8.readString(in); assertEquals("The int[] written by ObjectWritable was not labelled as " + "an ArrayPrimitiveWritable.Internal", ArrayPrimitiveWritable.Internal.class.getName(), className); ArrayPrimitiveWritable.Internal apwi = new ArrayPrimitiveWritable.Internal(); apwi.readFields(in); assertEquals("The ArrayPrimitiveWritable.Internal component type was corrupted", int.class, apw.getComponentType()); assertTrue("The int[] written by ObjectWritable as " + "ArrayPrimitiveWritable.Internal was corrupted", Arrays.equals(i, (int[])(apwi.get()))); //Read the APW object as written by ObjectWritable, but //"going around" ObjectWritable String declaredClassName = UTF8.readString(in); assertEquals("The APW written by ObjectWritable was not labelled as " + "declaredClass ArrayPrimitiveWritable", ArrayPrimitiveWritable.class.getName(), declaredClassName); className = UTF8.readString(in); assertEquals("The APW written by ObjectWritable was not labelled as " + "class ArrayPrimitiveWritable", ArrayPrimitiveWritable.class.getName(), className); ArrayPrimitiveWritable apw2 = new ArrayPrimitiveWritable(); apw2.readFields(in); assertEquals("The ArrayPrimitiveWritable component type was corrupted", int.class, apw2.getComponentType()); assertTrue("The int[] written by ObjectWritable as " + "ArrayPrimitiveWritable was corrupted", Arrays.equals(i, (int[])(apw2.get())));&#125; 下表是ObjectWritable.writeObject序列化ArrayPrimitiveWritable得到的byte信息（忽略直接写入数组的那部分数据）。 起始位置 长度 内容 备注 75 2 43 字符数组被封装在ArrayPrimitiveWritable中，记录类全名字符串的长度 77 43 “org.apache.hadoop.io. ArrayPrimitiveWritable” declaredClass.getName() 120 2 43 再一次写入类名字符串的长度 122 43 “org.apache.hadoop.io. ArrayPrimitiveWritable” instance.getClass().getName()，这里和declaredClass.getName()相同 165 2 3 Int数组元素为int型，”int”字符串的长度 167 3 “int” 数组的基本元素类型”int”字符串 170 4 3 数组总共有3个元素1、2、3，记录数组元素的个数，也就是数组长度 174 4 1 第一个元素1 178 4 2 第二个元素2 182 4 3 第三个元素3 写入结束，可以看到，ObjectWritable.writeObject处理Writable对象时，会将类名的字符串写入DataOutput2次 测试的后半部分，并没有用ObjectWritable.readObject去解析数据，而是用了对应的ArrayPrimitiveWritable$Internal和ArrayPrimitiveWritable直接解析，得到的结果和写入的一直，这里也证明了ObjectWritable.writeObject会隐式调用ArrayPrimitiveWritable$Internal和ArrayPrimitiveWritable。 到此ObjectWritable如何将Java类转换为byte流的，就介绍的差不多了。]]></content>
      <categories>
        <category>Java</category>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Java</tag>
      </tags>
  </entry>
</search>
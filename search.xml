<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Hadoop SequenceFile I/O 流分析]]></title>
    <url>%2F2017%2F08%2F25%2Fhadoop-sequencefile-class-analysis%2F</url>
    <content type="text"><![CDATA[由于SequenceFile中各种Stream相互嵌套，初学者很难理清各个Stream之间的关系，本文将直接展示Writer和Reader是如何构造并且初始化的，将嵌套的初始化过程展开成一个顺序的流程，让读者可以可以一目了然，并且展示最终构成层层类Stream字段的实际对象。这里将以RecordCompressWriter为例。 Writer类首先初始化一个DataOutputBuffer： 12345678910111213141516171819202122class DataOutputBuffer extends DataOutputStream &#123; class Buffer extends ByteArrayOutputStream &#123; Buffer() &#123; ByteArrayOutputStream() &#123; ByteArrayOutputStream(32) extends OutputStream &#123; protected byte buf[] = new byte[32]; &#125;; &#125;; &#125;; &#125; DataOutputBuffer() &#123; Buffer buffer = new Buffer(); DataOutputBuffer(buffer) &#123; DataOutputStream(buffer) extends FilterOutputStream &#123; FilterOutputStream(buffer) extends OutputStream &#123; protected OutputStream out = buffer; &#125;; &#125;; private Buffer buffer = buffer; &#125;; &#125;;&#125; 之后，使用创建一个FSDataOutputStream打开一个对应的FileOutputStream用来控制文件的写操作： 12345678910111213141516171819202122232425class FSDataOutputStream extends DataOutputStream &#123; LocalFSFileOutputStream localFSFileOutputStream = new LocalFSFileOutputStream(..) extends OutputStream &#123; private FileOutputStream fos = new FileOutputStream(..) extends OutputStream &#123; &#125;; &#125;; BufferedOutputStream bufferedOutputStream = new BufferedOutputStream(localFSFileOutputStream, ..) extends FilterOutputStream &#123; FilterOutputStream(localFSFileOutputStream) extends OutputStream &#123; protected OutputStream out = localFSFileOutputStream; &#125;; protected byte buf[] = new byte[4096]; &#125;; FSDataOutputStream(bufferedOutputStream, ..) &#123; PositionCache positionCache = new PositionCache(bufferedOutputStream, ..) extends FilterOutputStream &#123; FilterOutputStream(bufferedOutputStream) &#123; protected OutputStream out = bufferedOutputStream; &#125;; &#125;; DataOutputStream(positionCache) extends FilterOutputStream &#123; FilterOutputStream(positionCache) extends OutputStream &#123; protected OutputStream out = positionCache; &#125;; &#125;; private final OutputStream wrappedStream = bufferedOutputStream; &#125;;&#125; 之后创建一个Serializer来序列化key： 12345678910111213141516171819class JavaSerialization &#123; class JavaSerializationSerializer &#123; open(buffer) &#123; // DataOutputBuffer ObjectOutputStream objectOutputStream = new ObjectOutputStream(buffer) extends OutputStream &#123; BlockDataOutputStream blockDataOutputStream = new BlockDataOutputStream(buffer) extends OutputStream &#123; private final OutputStream out = buffer; DataOutputStream dataOutputStream = new DataOutputStream(blockDataOutputStream) extends FilterOutputStream &#123; FilterOutputStream(blockDataOutputStream) extends OutputStream &#123; protected OutputStream out = blockDataOutputStream; &#125;; &#125;; private final DataOutputStream dout = dataOutputStream; &#125;; private final BlockDataOutputStream bout = blockDataOutputStream; &#125;; private ObjectOutputStream oos = objectOutputStream; &#125;; &#125;&#125; 由于RecordCompressWriter会将value序列化后的结果做压缩后写入文件，故需要创建一个流来操作压缩过程： 1234567891011class DefaultCodec &#123; CompressionOutputStream createOutputStream(buffer, compressor) &#123; //DataOutputBuffer, BuiltInZlibDeflater CompressorStream(buffer, compressor) extends CompressionOutputStream &#123; CompressionOutputStream(buffer) extends OutputStream &#123; protected final OutputStream out = buffer; &#125;; protected Compressor compressor = compressor; protected byte[] buffer = new byte[4096]; &#125;; &#125;;&#125; 创建一个DataOutputStream来暂存value序列化后、压缩前的结果： 123456789101112131415class DataOutputStream extends FilterOutputStream &#123; BufferedOutputStream bufferedOutputStream = BufferedOutputStream(deflateFilter) &#123; // CompressorStream BufferedOutputStream(deflateFilter, ..) extends FilterOutputStream &#123; FilterOutputStream(deflateFilter) extends OutputStream &#123; protected OutputStream out = deflateFilter; &#125;; protected byte buf[] = new byte[8192]; &#125;; &#125;; DataOutputStream(bufferedOutputStream) &#123; FilterOutputStream(bufferedOutputStream) extends OutputStream &#123; protected OutputStream out = bufferedOutputStream; &#125;; &#125;;&#125; 创建一个Serializer来将value序列化： 12345678910111213141516171819class JavaSerialization &#123; class JavaSerializationSerializer &#123; open(deflateOut) &#123; // DataOutputStream ObjectOutputStream objectOutputStream = new ObjectOutputStream(deflateOut) extends OutputStream &#123; BlockDataOutputStream blockDataOutputStream = new BlockDataOutputStream(deflateOut) extends OutputStream &#123; private final OutputStream out = deflateOut; DataOutputStream dataOutputStream = new DataOutputStream(blockDataOutputStream) extends FilterOutputStream &#123; FilterOutputStream(blockDataOutputStream) extends OutputStream &#123; protected OutputStream out = blockDataOutputStream; &#125;; &#125;; private final DataOutputStream dout = dataOutputStream; &#125;; private final BlockDataOutputStream bout = blockDataOutputStream; &#125;; private ObjectOutputStream oos = objectOutputStream; &#125;; &#125;&#125; 最后writer由以上的这些流共同组成来完成写入文件的过程： 1234567891011class Writer &#123; FSDataOutputStream out; // FSDataOutputStream DataOutputBuffer buffer = new DataOutputBuffer(); // DataOutputBuffer CompressionCodec codec = null; // DefaultCodec CompressionOutputStream deflateFilter = null; // CompressorStream DataOutputStream deflateOut = null; // DataOutputStream Compressor compressor = null; // BuiltInZlibDeflater protected Serializer keySerializer; // JavaSerialization$JavaSerializationSerializer protected Serializer uncompressedValSerializer; // JavaSerialization$JavaSerializationSerializer protected Serializer compressedValSerializer; // JavaSerialization$JavaSerializationSerializer&#125; TestSequenceFileAppend测试类有测试append的方法： 12345678910111213141516171819202122232425@Testpublic void testAppend() throws Exception &#123; Path file = new Path(ROOT_PATH, "testseqappend.seq"); System.out.println(file); fs.delete(file, true); Text key1 = new Text("Key1"); Text value1 = new Text("Value1"); Text value2 = new Text("Updated"); SequenceFile.Metadata metadata = new SequenceFile.Metadata(); metadata.set(key1, value1); Writer.Option metadataOption = Writer.metadata(metadata); Writer writer = SequenceFile.createWriter(conf, SequenceFile.Writer.file(file), SequenceFile.Writer.keyClass(Long.class), SequenceFile.Writer.valueClass(String.class), metadataOption); writer.append(1L, "one"); writer.append(2L, "two"); writer.close(); verify2Values(file);&#125; SequenceFile使用append方法将key、value对写进文件中： 12345678910111213141516171819202122232425262728293031@Override@SuppressWarnings("unchecked")public synchronized void append(Object key, Object val) throws IOException &#123; if (key.getClass() != keyClass) throw new IOException("wrong key class: "+key.getClass().getName() +" is not "+keyClass); if (val.getClass() != valClass) throw new IOException("wrong value class: "+val.getClass().getName() +" is not "+valClass); buffer.reset(); // Append the 'key' keySerializer.serialize(key); int keyLength = buffer.getLength(); if (keyLength &lt; 0) throw new IOException("negative length keys not allowed: " + key); // Compress 'value' and append it deflateFilter.resetState(); compressedValSerializer.serialize(val); deflateOut.flush(); deflateFilter.finish(); // Write the record out checkAndWriteSync(); // sync out.writeInt(buffer.getLength()); // total record length out.writeInt(keyLength); // key portion length out.write(buffer.getData(), 0, buffer.getLength()); // data&#125; append方法首先将key序列化，序列化后的字节流由于不需要压缩，直接存放在DataOutputBuffer中，下表为将key值为1L写入DataOutputBuffer后的字符流： 起始位置 长度 内容 备注 0 1 ‘y’ TC_RESET 1 1 ‘s’ TC_OBJECT 2 1 ‘r’ TC_CLASSDESC 3 2 14 key的类型java.lang.Long的长度 5 14 “java.lang.Long” key的类型 19 8 4290774380558885855 getSerialVersionUID()，Long类型的序列号 27 1 2 SC_SERIALIZABLE 28 2 1 共序列化Long类型里面一个field 30 1 ‘J’ field的TypeCode 31 2 5 Long类型的value这个filed的字符串长度 33 5 “value” Long类型的名为value的field 38 1 ‘x’ TC_ENDBLOCKDATA 39 1 ‘r’ TC_CLASSDESC 40 2 16 Long父类java.lang.Number类型字符串的长度 42 16 “java.lang.Number” Long父类Number类型字符串 58 8 -8742448824652078965 getSerialVersionUID()，Number类型的序列号 66 1 2 SC_SERIALIZABLE 67 2 0 共序列化Number类型里面零个field 69 1 ‘x’ TC_ENDBLOCKDATA 70 1 ‘p’ TC_NULL 71 8 1 key的值 之后再将value序列化，暂存在deflateOut中，待全部序列化完成后，将deflateOut中的数据flush至deflateFilter中，deflateFilter是一个CompressorStream，在其中有一个compressor，会获取到所有未压缩的数据： 起始位置 长度 内容 备注 0 1 ‘y’ TC_RESET 1 1 ‘t’ TC_STRING 2 2 3 value “one”字符串的长度 4 3 “one” value “one”字符串 compressor会将上述字节流压缩为15个字节，之后写入DataOutputBuffer： 起始位置 长度 内容 备注 79 15 120，-100，-85，44，97，96，-50，-49，75，5，0，8，-88，2，51， value压缩结果 当一对key、value写进DataOutputBuffer后，FSDataOutputStream负责将暂存在DataOutputBuffer中的内容写进文件，在写字符流之前，FSDataOutputStream会将整个record的长度以及record的key的长度写进文件。 append测试中第二个key、value对的写入与第一对并无差异，就不过多介绍，下面开始讲解Reader的构成。 Reader类首先初始化一个DataOutputBuffer： 12345678910111213141516171819202122class DataOutputBuffer extends DataOutputStream &#123; class Buffer extends ByteArrayOutputStream &#123; Buffer() &#123; ByteArrayOutputStream() &#123; ByteArrayOutputStream(32) extends OutputStream &#123; protected byte buf[] = new byte[32]; &#125;; &#125;; &#125;; &#125; DataOutputBuffer() &#123; Buffer buffer = new Buffer(); DataOutputBuffer(buffer) &#123; DataOutputStream(buffer) extends FilterOutputStream &#123; FilterOutputStream(buffer) extends OutputStream &#123; protected OutputStream out = buffer; &#125;; &#125;; private Buffer buffer = buffer; &#125;; &#125;;&#125; 之后，使用创建一个FSDataInputStream打开一个对应的FileOutputStream用来控制文件的写操作： 123456789101112131415161718192021class FSDataInputStream extends DataInputStream &#123; LocalFSFileInputStream localFSFileInputStream = new LocalFSFileInputStream(..) extends FSInputStream &#123; private FileInputStream fis = new FileInputStream(..) extends InputStream &#123; &#125;; &#125;; BufferedFSInputStream bufferedFSInputStream = new BufferedFSInputStream(localFSFileInputStream, ..) extends BufferedInputStream &#123; BufferedInputStream(localFSFileInputStream, ..) extends FilterInputStream &#123; FilterInputStream(localFSFileInputStream) extends InputStream &#123; protected InputStream in = localFSFileInputStream; &#125;; protected byte buf[] = new byte[4096]; &#125;; &#125;; FSDataInputStream(bufferedFSInputStream, ..) &#123; DataInputStream(bufferedFSInputStream) extends FilterInputStream &#123; FilterInputStream(bufferedFSInputStream) extends InputStream &#123; protected InputStream in = bufferedFSInputStream; &#125;; &#125;; &#125;;&#125; 之后创建一个DataInputBuffer赋给valBuffer： 1234567891011121314151617181920class DataInputBuffer extends DataInputStream &#123; class Buffer extends ByteArrayInputStream &#123; Buffer() &#123; ByteArrayInputStream(new byte[] &#123;&#125;) extends OutputStream &#123; protected byte buf[] = &#123;&#125;; &#125;; &#125;; &#125; DataInputBuffer() &#123; Buffer buffer = new Buffer(); DataInputBuffer(buffer) &#123; DataInputStream(buffer) extends FilterInputStream &#123; FilterInputStream(buffer) extends InputStream &#123; protected volatile InputStream in = buffer; &#125;; &#125;; private Buffer buffer = buffer; &#125;; &#125;;&#125; 构造一个DecompressorStream赋给valInFilter，valInFilter将会从valBuffer中读取压缩后的value值，而后解压： 1234567891011class DefaultCodec &#123; CompressionInputStream createInputStream(valBuffer, valDecompressor) &#123; //DataInputBuffer, BuiltInZlibInflater DecompressorStream(valBuffer, valDecompressor) extends CompressionInputStream &#123; CompressionInputStream(valBuffer) extends InputStream &#123; protected final InputStream in = valBuffer; &#125;; protected Decompressor decompressor = valDecompressor; protected byte[] buffer = new byte[4096]; &#125;; &#125;;&#125; 用valInFilter构造一个DataInputStream赋给valIn，从而valIn可以从valInFilter拿到解压后的value值： 12345class DataInputStream extends FilterInputStream &#123; FilterOutputStream(valInFilter) extends InputStream &#123; protected volatile InputStream in = valInFilter; &#125;;&#125; 用valBuffer构造一个JavaSerializationDeserializer赋给keyDeserializer，keyDeserializer将会从valBuffer中直接解析出key的值： 12345678910111213141516171819202122class JavaSerialization &#123; class JavaSerializationDeserializer &#123; open(valBuffer) &#123; // DataInputBuffer ObjectInputStream objectInputStream = new ObjectInputStream(valBuffer) extends InputStream &#123; BlockDataInputStream blockDataInputStream = new BlockDataInputStream(valBuffer) extends InputStream &#123; PeekInputStream peekInputStream = new PeekInputStream(valBuffer) extends InputStream &#123; private final InputStream in = valBuffer; &#125; private final PeekInputStream in = peekInputStream; DataInputStream dataInputStream = new DataInputStream(blockDataInputStream) extends FilterInputStream &#123; FilterInputStream(blockDataInputStream) extends InputStream &#123; protected volatile InputStream in = blockDataInputStream; &#125;; &#125;; private final DataInputStream din = dataInputStream; &#125;; private final BlockDataInputStream bin = blockDataInputStream; &#125;; private ObjectInputStream ois = objectInputStream; &#125;; &#125;&#125; 用valIn构造一个JavaSerializationDeserializer赋给valDeserializer，valDeserializer将会从valIn中获取到最终的value值： 12345678910111213141516171819202122class JavaSerialization &#123; class JavaSerializationDeserializer &#123; open(valIn) &#123; // DataInputStream ObjectInputStream objectInputStream = new ObjectInputStream(valIn) extends InputStream &#123; BlockDataInputStream blockDataInputStream = new BlockDataInputStream(valIn) extends InputStream &#123; PeekInputStream peekInputStream = new PeekInputStream(valIn) extends InputStream &#123; private final InputStream in = valIn; &#125; private final PeekInputStream in = peekInputStream; DataInputStream dataInputStream = new DataInputStream(blockDataInputStream) extends FilterInputStream &#123; FilterInputStream(blockDataInputStream) extends InputStream &#123; protected volatile InputStream in = blockDataInputStream; &#125;; &#125;; private final DataInputStream din = dataInputStream; &#125;; private final BlockDataInputStream bin = blockDataInputStream; &#125;; private ObjectInputStream ois = objectInputStream; &#125;; &#125;&#125; 最后reader由以上的这些流共同组成来完成读入文件的过程： 12345678910111213141516171819202122232425class Reader &#123; FSDataInputStream in; // FSDataInputStream DataOutputBuffer outBuf = new DataOutputBuffer(); // DataOutputBuffer CompressionCodec codec = null; // DefaultCodec DataInputBuffer keyLenBuffer = null; CompressionInputStream keyLenInFilter = null; DataInputStream keyLenIn = null; Decompressor keyLenDecompressor = null; DataInputBuffer keyBuffer = null; CompressionInputStream keyInFilter = null; DataInputStream keyIn = null; Decompressor keyDecompressor = null; DataInputBuffer valLenBuffer = null; CompressionInputStream valLenInFilter = null; DataInputStream valLenIn = null; Decompressor valLenDecompressor = null; DataInputBuffer valBuffer = null; // DataInputBuffer CompressionInputStream valInFilter = null; // DecompressorStream DataInputStream valIn = null; // DataInputStream Decompressor valDecompressor = null; // BuiltInZlibInflater Deserializer keyDeserializer; // JavaSerialization$JavaSerializationDeserializer Deserializer valDeserializer; // JavaSerialization$JavaSerializationDeserializer&#125;]]></content>
      <categories>
        <category>Java</category>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Java</tag>
        <tag>I/O</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop ObjectWritable 源码与相关单元测试介绍]]></title>
    <url>%2F2017%2F07%2F29%2Fhadoop-objectwritable-class-analysis%2F</url>
    <content type="text"><![CDATA[Hadoop IO模块是序列化和RPC的基础，该模块负责将java的各种对象转化为byte流以便在网络上传输，并且还要负责将byte流转回原对象。开篇我们先看一下hadoop IO模块类的关系图： 其他的很好理解，每种java的原生的类和容器都有一个对应实现了Writable的类（IntWritable、MapFile等）。ObjectWritable 是hadoop.io模块里面一个比较特殊的类（用红色标出），它是一个通用的Writable类，可以将任意的自定义类转为byte流，原因是因为会它会把将要转换的对象的类信息记录在自己的成员变量private Class declaredClass;中，然后序列化时，将类的名字，以string的形式一并写入byte流中。在收到对应的byte流后，解析程序可以知道类名string的长度，然后从byte流中获得类的信息，之后就是根据类的信息去以此解析这个类的所有成员变量的值。 ObjectWritable是Writable的实现，必然要实现接口的方法： void write(DataOutput out) throws IOException;和void readFields(DataInput in) throws IOException;。ObjectWritable实现是通过调用类中的静态方法。 12345678@Overridepublic void write(DataOutput out) throws IOException &#123; writeObject(out, instance, declaredClass, conf);&#125;@Overridepublic void readFields(DataInput in) throws IOException &#123; readObject(in, this, this.conf);&#125; 所以ObjectWritable最主要的逻辑集中在这2个静态函数上，先看writeObject方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384 /** * Write a &#123;@link Writable&#125;, &#123;@link String&#125;, primitive type, or an array of * the preceding. * * @param allowCompactArrays - set true for RPC and internal or intra-cluster * usages. Set false for inter-cluster, File, and other persisted output * usages, to preserve the ability to interchange files with other clusters * that may not be running the same version of software. Sometime in ~2013 * we can consider removing this parameter and always using the compact format. * 入参包含要写入的流，要序列化的对象，对象的类信息，以及配置信息和是否允许封装数组 */ public static void writeObject(DataOutput out, Object instance, Class declaredClass, Configuration conf, boolean allowCompactArrays) throws IOException &#123; if (instance == null) &#123; // null instance = new NullInstance(declaredClass, conf); declaredClass = Writable.class; &#125; // Special case: must come before writing out the declaredClass. // If this is an eligible array of primitives, // wrap it in an ArrayPrimitiveWritable$Internal wrapper class. // // 当允许封装数组时，并且对象是一个只包含primitives的数组，就将对象数组包裹在 // ArrayPrimitiveWritable.Internal这个类中，同时修改对象的类信息为 // ArrayPrimitiveWritable.Internal.class if (allowCompactArrays &amp;&amp; declaredClass.isArray() &amp;&amp; instance.getClass().getName().equals(declaredClass.getName()) &amp;&amp; instance.getClass().getComponentType().isPrimitive()) &#123; instance = new ArrayPrimitiveWritable.Internal(instance); declaredClass = ArrayPrimitiveWritable.Internal.class; &#125; // 将类的名称写进byte流中：为了将来可以成功的将类的名称解析出来，writeString还会将 // string的长度以short类型写在名称之前 UTF8.writeString(out, declaredClass.getName()); // always write declared // 如果是数组，但是没有被封装在ArrayPrimitiveWritable.Internal类中，就递归调用 // writeObject写数组中的每一个元素 if (declaredClass.isArray()) &#123; // non-primitive or non-compact array int length = Array.getLength(instance); out.writeInt(length); for (int i = 0; i &lt; length; i++) &#123; writeObject(out, Array.get(instance, i), declaredClass.getComponentType(), conf, allowCompactArrays); &#125; // 如果数据已经被封装在了ArrayPrimitiveWritable.Internal中，那么就调用 // ArrayPrimitiveWritable.Internal的write方法 &#125; else if (declaredClass == ArrayPrimitiveWritable.Internal.class) &#123; ((ArrayPrimitiveWritable.Internal) instance).write(out); // 如果是String类型，调用UTF8的writeString法法 &#125; else if (declaredClass == String.class) &#123; // String UTF8.writeString(out, (String)instance); // 如果是基本类型，则直接调用DataOutput的相应write方法 &#125; else if (declaredClass.isPrimitive()) &#123; // primitive type if (declaredClass == Boolean.TYPE) &#123; // boolean out.writeBoolean(((Boolean)instance).booleanValue()); &#125; else if (declaredClass == Character.TYPE) &#123; // char out.writeChar(((Character)instance).charValue()); &#125; else if (declaredClass == Byte.TYPE) &#123; // byte out.writeByte(((Byte)instance).byteValue()); &#125; ...... else if (declaredClass == Void.TYPE) &#123; // void &#125; else &#123; throw new IllegalArgumentException("Not a primitive: "+declaredClass); &#125; &#125; else if (declaredClass.isEnum()) &#123; // enum UTF8.writeString(out, ((Enum)instance).name()); // 如果类是Writable的衍生类，则先将对象的类的名字写入byte流中，然后再调用Writable的write方法 // 由于在函数的开始，已经将declaredClass字符串写进了byte流，这里还会再写一次，不过这次的方法 // 写入的内容由instance.getClass()决定的，故可以推断存在可能declaredClass和 // instance.getClass()获得的内容不一样，需要再次将类型的string写入 &#125; else if (Writable.class.isAssignableFrom(declaredClass)) &#123; // Writable UTF8.writeString(out, instance.getClass().getName()); ((Writable)instance).write(out); &#125; else if (Message.class.isAssignableFrom(declaredClass)) &#123; ((Message)instance).writeDelimitedTo( DataOutputOutputStream.constructOutputStream(out)); &#125; else &#123; throw new IOException("Can't write: "+instance+" as "+declaredClass); &#125;&#125; 接下来看看ArrayPrimitiveWritable.Internal的write方法，逻辑很简单，1）先将数组中存储的基本类型名称以string的方式写到byte流中（包括类名称string的长度）；2）将数组中元素的个数以Int的方式写进流中；3）然后根据基本类型来调用相应的写数组方法（writeBooleanArray、writeIntArray等）将数组写进流中。writeIntArray等就是简单的遍历各个元素，然后调用DataOutput的write基本类型的方法writeInt。 123456789101112131415161718192021222324252627282930313233343536373839/* * @see org.apache.hadoop.io.Writable#write(java.io.DataOutput) */@Override@SuppressWarnings("deprecation")public void write(DataOutput out) throws IOException &#123; // write componentType UTF8.writeString(out, componentType.getName()); // write length out.writeInt(length); // do the inner loop. Walk the decision tree only once. if (componentType == Boolean.TYPE) &#123; // boolean writeBooleanArray(out); &#125; else if (componentType == Character.TYPE) &#123; // char writeCharArray(out); &#125; else if (componentType == Byte.TYPE) &#123; // byte writeByteArray(out); &#125; else if (componentType == Short.TYPE) &#123; // short writeShortArray(out); &#125; else if (componentType == Integer.TYPE) &#123; // int writeIntArray(out); &#125; else if (componentType == Long.TYPE) &#123; // long writeLongArray(out); &#125; else if (componentType == Float.TYPE) &#123; // float writeFloatArray(out); &#125; else if (componentType == Double.TYPE) &#123; // double writeDoubleArray(out); &#125; else &#123; throw new IOException("Component type " + componentType.toString() + " is set as the output type, but no encoding is implemented for this type."); &#125;&#125;private void writeIntArray(DataOutput out) throws IOException &#123; int[] v = (int[]) value; for (int i = 0; i &lt; length; i++) out.writeInt(v[i]);&#125; 至此就已经完成了将一个对象写到byte流中，接着看看从byte流中解析获得对象的过程readObject。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182/** Read a &#123;@link Writable&#125;, &#123;@link String&#125;, primitive type, or an array of * the preceding. */@SuppressWarnings("unchecked")public static Object readObject(DataInput in, ObjectWritable objectWritable, Configuration conf) throws IOException &#123; // 函数开始，就会尝试将对象的类名解析出来：byte流的头2个字节（short）表示了类的名称的string长度 // 然后根据这个长度读取接下来的一定字节数就可以成功的获得类名 String className = UTF8.readString(in); // 如果是基本类型，那就直接可以将类名的string转换为Java的类对象 Class&lt;?&gt; declaredClass = PRIMITIVE_NAMES.get(className); // 如果不是基本类型，则调用loadClass方法 if (declaredClass == null) &#123; declaredClass = loadClass(conf, className); &#125; Object instance; // 如果对象是基本类型，那就调用DataInput对应的read方法 if (declaredClass.isPrimitive()) &#123; // primitive types if (declaredClass == Boolean.TYPE) &#123; // boolean instance = Boolean.valueOf(in.readBoolean()); &#125; else if (declaredClass == Character.TYPE) &#123; // char instance = Character.valueOf(in.readChar()); &#125; else if (declaredClass == Byte.TYPE) &#123; // byte instance = Byte.valueOf(in.readByte()); &#125; else if (declaredClass == Short.TYPE) &#123; // short instance = Short.valueOf(in.readShort()); &#125; ...... &#125; else if (declaredClass == Void.TYPE) &#123; // void instance = null; &#125; else &#123; throw new IllegalArgumentException("Not a primitive: "+declaredClass); &#125; // 如果是没有被封装在ArrayPrimitiveWritable.Internal中的数组，就递归调用readObject // 依次读取数组的各个元素 &#125; else if (declaredClass.isArray()) &#123; // array int length = in.readInt(); instance = Array.newInstance(declaredClass.getComponentType(), length); for (int i = 0; i &lt; length; i++) &#123; Array.set(instance, i, readObject(in, conf)); &#125; // 如果被封装在了ArrayPrimitiveWritable.Internal类中，那么就调用 // ArrayPrimitiveWritable.Internal的readFields方法，然后保存在instance局部变量中 &#125; else if (declaredClass == ArrayPrimitiveWritable.Internal.class) &#123; // Read and unwrap ArrayPrimitiveWritable$Internal array. // Always allow the read, even if write is disabled by allowCompactArrays. ArrayPrimitiveWritable.Internal temp = new ArrayPrimitiveWritable.Internal(); temp.readFields(in); instance = temp.get(); declaredClass = instance.getClass(); // 其余的读取方法大同小异，都是找到该类对应的read方法 &#125; else if (declaredClass == String.class) &#123; // String instance = UTF8.readString(in); &#125; else if (declaredClass.isEnum()) &#123; // enum instance = Enum.valueOf((Class&lt;? extends Enum&gt;) declaredClass, UTF8.readString(in)); &#125; else if (Message.class.isAssignableFrom(declaredClass)) &#123; instance = tryInstantiateProtobuf(declaredClass, in); // 如果类型是Writable或Writable的衍生类，则执行下列代码 // 由于writeObject的write逻辑，将类型写入byte流了2次，故这里需要再次读取一下确定instance的类型 &#125; else &#123; // Writable Class instanceClass = null; String str = UTF8.readString(in); instanceClass = loadClass(conf, str); Writable writable = WritableFactories.newInstance(instanceClass, conf); writable.readFields(in); instance = writable; if (instanceClass == NullInstance.class) &#123; // null declaredClass = ((NullInstance)instance).declaredClass; instance = null; &#125; &#125; // 到这里可以将declaredClass和instance赋值给参数传入的ObjectWritable对象 if (objectWritable != null) &#123; // store values objectWritable.declaredClass = declaredClass; objectWritable.instance = instance; &#125; return instance; &#125; 解析过程到此就完成了。序列化和反序列化过程都是一一对应的。接下来就用Hadoop自带的测试用例来验证运行一下上述代码，看看写入byte流中的内容。测试类为TestArrayPrimitiveWritable。这个类主要测试了ArrayPrimitiveWritable的write和readFields方法。这2个方法基本覆盖了其他的Writable和其衍生类的读写功能。 123456789101112131415161718192021222324252627282930313233343536static final char[] c = &#123;'a', 'b', 'c'&#125;;static final int[] i = &#123;1, 2, 3&#125;;static final Object[] bigSet = &#123;c, i&#125;;static final Object[] expectedResultSet = &#123;c, c, i, i&#125;;@Testpublic void testMany() throws IOException &#123; //Write a big set of data, one of each primitive type array for (Object x : bigSet) &#123; //write each test object two ways //First, transparently via ObjectWritable ObjectWritable.writeObject(out, x, x.getClass(), null, true); //Second, explicitly via ArrayPrimitiveWritable (new ArrayPrimitiveWritable(x)).write(out); &#125; //Now read the data back in in.reset(out.getData(), out.getLength()); for (int x = 0; x &lt; resultSet.length; ) &#123; //First, transparently resultSet[x++] = ObjectWritable.readObject(in, null); //Second, explicitly ArrayPrimitiveWritable apw = new ArrayPrimitiveWritable(); apw.readFields(in); resultSet[x++] = apw.get(); &#125; //validate data structures and values assertEquals(expectedResultSet.length, resultSet.length); for (int x = 0; x &lt; resultSet.length; x++) &#123; assertEquals("ComponentType of array " + x, expectedResultSet[x].getClass().getComponentType(), resultSet[x].getClass().getComponentType()); &#125; assertTrue("In and Out arrays didn't match values", Arrays.deepEquals(expectedResultSet, resultSet));&#125; 下表详细列出了bigSet由ObjectWritable和ArrayPrimitiveWritable写入DataOutput中的数据。 起始位置 长度 内容 备注 0 2 52 字符数组被封装在ArrayPrimitiveWritable.Internal中，记录类全名字符串的长度 2 52 “org.apache.hadoop.io. ArrayPrimitiveWritable$Internal” declaredClass.getName() 54 2 4 数组元素为char型，”char”字符串的长度 56 4 “char” 数组的基本元素类型”char”字符串 60 4 3 数组总共有3个元素’a’、’b’、’c’，记录数组元素的个数，也就是数组长度 64 2 ‘a’ 第一个元素’a’ 66 2 ‘b’ 第二个元素’b’ 68 2 ‘c’ 第三个元素’c’ ObjectWritable.writeObject写char数组结束，接下来是直接调用ArrayPrimitiveWritable来写char数组 70 2 4 数组元素为char型，”char”字符串的长度 72 4 “char” 数组的基本元素类型”char”字符串 76 4 3 数组总共有3个元素’a’、’b’、’c’，记录数组元素的个数，也就是数组长度 80 2 ‘a’ 第一个元素’a’ 82 2 ‘b’ 第二个元素’b’ 84 2 ‘c’ 第三个元素’c’ ArrayPrimitiveWritable写char数组完成，开始用ObjectWritable.writeObject写Int数组 86 2 52 Int数组同样的被封装在ArrayPrimitiveWritable.Internal中，记录类全名字符串的长度 88 52 “org.apache.hadoop.io. ArrayPrimitiveWritable$Internal” declaredClass.getName() 140 2 3 Int数组元素为int型，”int”字符串的长度 142 3 “int” 数组的基本元素类型”int”字符串 145 4 3 数组总共有3个元素1、2、3，记录数组元素的个数，也就是数组长度 149 4 1 第一个元素1 153 4 2 第二个元素2 157 4 3 第三个元素3 ObjectWritable.writeObject写Int数组结束，接下来是直接调用ArrayPrimitiveWritable来写Int数组 161 2 3 Int数组元素为int型，”int”字符串的长度 163 3 “int” 数组的基本元素类型”int”字符串 166 4 3 数组总共有3个元素1、2、3，记录数组元素的个数，也就是数组长度 170 4 1 第一个元素1 174 4 2 第二个元素2 178 4 3 第三个元素3 写入流程结束，DataOutput中总共有182个字节数据 将这个182个字节传递给DataInput对象，然后分别用ObjectWritable.readObject和ArrayPrimitiveWritable处理，可以准确的将2个字符串的2遍都读取出来，最后放进resultSet中，将resultSet和expectedResultSet对比，可以发现最终结果和期望的结果一致，表明解析数组成功了。 从这182字节的内容，我们可以看出ObjectWritable.writeObject比ArrayPrimitiveWritable只是多了传递给ObjectWritable.writeObject的declaredClass字符串的长度，其他都是一样的，调用ObjectWritable.writeObject会隐式调用ArrayPrimitiveWritable的write方法。 这个测试用例只测试了ObjectWritable.writeObject处理元素为基本类型的数组，并没有测试处理Writable类型的对象。对于Writable对象，其本身已经包含了序列化的函数write和readFields，并不需要借助ObjectWritable来实现序列化，但ObjectWritable可以处理任何类的序列化需求，当然也不排除处理Writable及它衍生类。 下面一个测试用例，可以用来检验ObjectWritable.writeObject处理Writable类型的对象。首先，使用ObjectWritable.writeObject直接序列化一个Int数组，之后，用ArrayPrimitiveWritable封装这个Int数组，再用ObjectWritable.writeObject序列化封装过后的这个数组。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152static final int[] i = &#123;1, 2, 3&#125;;@Test@SuppressWarnings("deprecation")public void testObjectLabeling() throws IOException &#123; //Do a few tricky experiments to make sure things are being written //the way we expect //Write the data array with ObjectWritable //which will indirectly write it using APW.Internal ObjectWritable.writeObject(out, i, i.getClass(), null, true); //Write the corresponding APW directly with ObjectWritable ArrayPrimitiveWritable apw = new ArrayPrimitiveWritable(i); ObjectWritable.writeObject(out, apw, apw.getClass(), null, true); //Get ready to read it back in.reset(out.getData(), out.getLength()); //Read the int[] object as written by ObjectWritable, but //"going around" ObjectWritable String className = UTF8.readString(in); assertEquals("The int[] written by ObjectWritable was not labelled as " + "an ArrayPrimitiveWritable.Internal", ArrayPrimitiveWritable.Internal.class.getName(), className); ArrayPrimitiveWritable.Internal apwi = new ArrayPrimitiveWritable.Internal(); apwi.readFields(in); assertEquals("The ArrayPrimitiveWritable.Internal component type was corrupted", int.class, apw.getComponentType()); assertTrue("The int[] written by ObjectWritable as " + "ArrayPrimitiveWritable.Internal was corrupted", Arrays.equals(i, (int[])(apwi.get()))); //Read the APW object as written by ObjectWritable, but //"going around" ObjectWritable String declaredClassName = UTF8.readString(in); assertEquals("The APW written by ObjectWritable was not labelled as " + "declaredClass ArrayPrimitiveWritable", ArrayPrimitiveWritable.class.getName(), declaredClassName); className = UTF8.readString(in); assertEquals("The APW written by ObjectWritable was not labelled as " + "class ArrayPrimitiveWritable", ArrayPrimitiveWritable.class.getName(), className); ArrayPrimitiveWritable apw2 = new ArrayPrimitiveWritable(); apw2.readFields(in); assertEquals("The ArrayPrimitiveWritable component type was corrupted", int.class, apw2.getComponentType()); assertTrue("The int[] written by ObjectWritable as " + "ArrayPrimitiveWritable was corrupted", Arrays.equals(i, (int[])(apw2.get())));&#125; 下表是ObjectWritable.writeObject序列化ArrayPrimitiveWritable得到的byte信息（忽略直接写入数组的那部分数据）。 起始位置 长度 内容 备注 75 2 43 字符数组被封装在ArrayPrimitiveWritable中，记录类全名字符串的长度 77 43 “org.apache.hadoop.io. ArrayPrimitiveWritable” declaredClass.getName() 120 2 43 再一次写入类名字符串的长度 122 43 “org.apache.hadoop.io. ArrayPrimitiveWritable” instance.getClass().getName()，这里和declaredClass.getName()相同 165 2 3 Int数组元素为int型，”int”字符串的长度 167 3 “int” 数组的基本元素类型”int”字符串 170 4 3 数组总共有3个元素1、2、3，记录数组元素的个数，也就是数组长度 174 4 1 第一个元素1 178 4 2 第二个元素2 182 4 3 第三个元素3 写入结束，可以看到，ObjectWritable.writeObject处理Writable对象时，会将类名的字符串写入DataOutput2次 测试的后半部分，并没有用ObjectWritable.readObject去解析数据，而是用了对应的ArrayPrimitiveWritable$Internal和ArrayPrimitiveWritable直接解析，得到的结果和写入的一直，这里也证明了ObjectWritable.writeObject会隐式调用ArrayPrimitiveWritable$Internal和ArrayPrimitiveWritable。 到此ObjectWritable如何将Java类转换为byte流的，就介绍的差不多了。]]></content>
      <categories>
        <category>Java</category>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Java</tag>
      </tags>
  </entry>
</search>